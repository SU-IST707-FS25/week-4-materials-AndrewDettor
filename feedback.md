# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** AndrewDettor
**Total Score:** 25/40 (62.5%)

**Grade Category:** D (Poor)

---

## Problem Breakdown

### Exercise 1 (7/16 = 43.8%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ Good start: you fit PCA and plotted the first two components colored by labels (assuming seaborn imported). To better match the task, set n_components (e.g., 2 or 50), report explained variance, and optionally reconstruct images via inverse_transform to visualize approximations.

**Part pipeline-part2** (pipeline-part2.code): 1/4 points

_Feedback:_ You plotted explained variance but did not visualize the 2D PCA scatter colored by class as required. Use your prior X_pca and target: sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=target). Reduction step can rely on earlier fit; just provide the scatter plot.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You didn’t produce the required scree plot for the first 40 components. Instead you computed cumulative variance and printed a slice. A correct solution should plot the first 40 values of pca.explained_variance_ratio_ on the y-axis (percent of variance explained).

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ You correctly used the previously computed n_components to fit PCA and apply/invert the transform. This appropriately applies the 95% variance selection from prior work. Minor tip: argmax on cumulative often needs +1 for component count, but no points off here.

**Part pipeline-part5** (pipeline-part5.code): 1/4 points

_Feedback:_ You didn’t visualize a digit from the reduced space. This step requires using the Step 4 n_components to reduce, inverse_transform a single digit, and call plot_mnist_digit on it. Your code runs PCA and KNN accuracy instead, uses 0.80 variance, and never plots a reconstructed dig

---

### Exercise 2 (9/10 = 90.0%)

**Part ex1-part1** (ex1-part1.code): 3/4 points

_Feedback:_ Good use of TSNE (defaults give 2D) and a scatter with labels via hue. However, you didn’t clearly use the MNIST variables from above (expected X_mnist_train/y_mnist_train). Add plt.show() and titles/colorbar for clarity. Ensure X and target are MNIST.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you used the TSNE embedding (X_tsne), split data, trained KNN, and reported accuracy—meets the task. Using y vs target is fine given prior context. For robustness, consider setting n_neighbors explicitly and using stratify in the split.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you fit UMAP on the train set, transformed the test set, trained KNN, and reported accuracy. This is a correct approach for computing KNN accuracy after DR. Ensure X_mnist_train/test and y variables are defined earlier in your notebook.

---

### Exercise 4 (9/14 = 64.3%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ Good start: you applied PCA (2D/3D), trained KNN, and visualized. Missing key parts: no UMAP, no UMAP parameter exploration, and limited dimensionality/neighbor sweeps. Add UMAP fits with varied n_neighbors/min_dist and compare KNN accuracy across dims, plus visualizations.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented UMAP+KNN and visualizations, which is a reasonable DR approach, but the task explicitly asked to try with PCA and to build on your prior PCA work. Not using PCA is a major miss. Minor: 3D plot labeled 'Original Data' is misleading. Use PCA here for full credit.

**Part ex2-part3** (ex2-part3.answer): 6/7 points

_Feedback:_ Good, concise rationale: if classes form a single blob with little manifold structure, PCA may outperform UMAP. To strengthen, reference your actual accuracy/plots and note that UMAP performance depends on parameters (n_neighbors, min_dist, n_components).

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:12 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*
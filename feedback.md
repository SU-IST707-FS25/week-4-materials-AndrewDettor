# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** AndrewDettor
**Raw Score:** 48/50 (96.0%)
**Course Points Earned:** 100.0

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Good: You ran t-SNE (2D by default) and plotted a 2D scatter colored by labels. This meets the goal. Minor suggestions: ensure X/target are the MNIST data intended and set random_state for reproducibility; optional: add title/legend/colorbar.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job. You split the TSNE-embedded data, trained KNN, and reported accuracy, directly addressing performance. You correctly used your prior X_tsne. Ensure y matches your earlier target variable, but no penalty here.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you used UMAP, fit on train and transformed test, then trained KNN and computed accuracy. This matches the task and proper workflow. Full credit.

---

### Exercise 4 (19/20 = 95.0%)

**Part ex2-part1** (ex2-part1.code): 7/7 points

_Feedback:_ Excellent: you applied PCA (2D and 3D), trained KNN on transformed data, reported accuracy, and visualized components. Approach is correct and complete. Minor nit: 3D plot title says ‘Original Data’ though it’s PCA-transformed.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Great job: you correctly applied UMAP (fit on train, transform test), trained KNN, reported accuracy, and visualized embeddings. Consider matching plot titles to the embedding and using 2D UMAP for clearer plots, but your implementation fully satisfies the task.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ You ran PCA and UMAP and did some parameter exploration. Your explanation is reasonable if your results showed PCA > UMAP. For full credit, note that UMAP often performs better in low dimensions with smaller n_neighbors; consider trying 2D and lower n_neighbors.

---

### Exercise 1 (19/20 = 95.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Good job: You applied PCA and plotted the first two components colored by class. The approach fulfills the requirements. Minor: adding a legend/title and ensuring consistent variable names with the dataset (e.g., MNIST) would improve clarity.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Good scree plot: uses pca.explained_variance_ratio_ and plots first 40 components with appropriate y as proportion/percent of variance explained. Matches prior work objects. Consider adding x/y labels or title for clarity, but not required here.

**Part pipeline-part3** (pipeline-part3.code): 3/4 points

_Feedback:_ Good approach using cumulative variance and argmax. However, you missed the +1 to convert the index to the number of components (zero-based index). Use: n_components = np.argmax(cumulative >= 0.95) + 1. Otherwise correct and consistent with prior work.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Good job. You used the n_components from Step 4, reduced with PCA, inverse-transformed, and visualized the reconstructed digit. This matches the task and should work given your prior variables. Minor: ensure X is the same dataset as earlier to keep the “same digit.”

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Well done. You trained KNN with and without PCA and used PCA(0.80) to preserve ~80% variance, then evaluated accuracies. Additional checks (cumulative variance, transformed shape) are appropriate. Meets the task’s objectives.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:51:36 UTC

If you have questions about your grade, please reach out to the instructor.